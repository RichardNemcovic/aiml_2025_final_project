{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226c22c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load tokenizer for IBM Granite model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")  # Local fallback for syntax compatibility\n",
    "\n",
    "# --- Helpers ---\n",
    "\n",
    "def regex_sent_tokenize(text):\n",
    "    \"\"\"Split text into sentences using punctuation boundaries.\"\"\"\n",
    "    sentences = re.split(r'(?<=[.!?])(?:\\s+|\\n+)', text)\n",
    "    return [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "def tokenizer_safe_chunking(text, max_tokens=450):\n",
    "    \"\"\"\n",
    "    Chunk text by tokens using the tokenizer (reserving 2 tokens for special tokens).\n",
    "    Tries to split at sentence boundaries.\n",
    "    \"\"\"\n",
    "    sentences = regex_sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    current_tokens = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_tokens = tokenizer.encode(sentence, add_special_tokens=False)\n",
    "        token_len = len(sentence_tokens)\n",
    "\n",
    "        # If adding this sentence would overflow, save current chunk\n",
    "        if current_tokens + token_len > max_tokens and current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence\n",
    "            current_tokens = token_len\n",
    "        else:\n",
    "            current_chunk += \" \" + sentence if current_chunk else sentence\n",
    "            current_tokens += token_len\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    # Final safety pass: trim any chunk still too long after decode\n",
    "    safe_chunks = []\n",
    "    for chunk in chunks:\n",
    "        token_ids = tokenizer.encode(chunk, add_special_tokens=True)\n",
    "        if len(token_ids) <= 450:\n",
    "            safe_chunks.append(chunk)\n",
    "        else:\n",
    "            # Hard split the long chunk into smaller ones\n",
    "            split_ids = tokenizer.encode(chunk, add_special_tokens=False)\n",
    "            for i in range(0, len(split_ids), max_tokens):\n",
    "                part_ids = split_ids[i:i+max_tokens]\n",
    "                safe_chunk = tokenizer.decode(part_ids, skip_special_tokens=True)\n",
    "                safe_chunks.append(safe_chunk.strip())\n",
    "\n",
    "    return safe_chunks\n",
    "\n",
    "with open(\"data/english_courses.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "chunked_data = []\n",
    "\n",
    "for entry in data:\n",
    "    description = str(entry.get(\"description\", \"\")).strip()\n",
    "    if not description:\n",
    "        continue\n",
    "\n",
    "    chunks = tokenizer_safe_chunking(description)\n",
    "\n",
    "    for idx, chunk in enumerate(chunks, start=1):\n",
    "        new_entry = {\n",
    "            \"chunk_number\": idx,\n",
    "            \"chunk_text\": chunk\n",
    "        }\n",
    "        # Include original metadata\n",
    "        for key in [\"programme\", \"course_title\", \"url\", \"language\", \"ects\", \"type\", \"level\", \"study_board\"]:\n",
    "            new_entry[key] = entry.get(key, \"\")\n",
    "\n",
    "        # Final token safety check\n",
    "        token_length = len(tokenizer.encode(chunk, add_special_tokens=True))\n",
    "        if token_length > 450:\n",
    "            print(f\"Still too long ({token_length} tokens): {entry.get('course_title', 'Unknown')}\")\n",
    "            continue\n",
    "\n",
    "        chunked_data.append(new_entry)\n",
    "\n",
    "# --- Save result ---\n",
    "with open(\"data/chunked_courses.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(chunked_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Created {len(chunked_data)} chunks and saved to 'chunked_courses.json'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
