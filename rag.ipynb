{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Organize imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decouple import config\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from instructor import Mode\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_ibm import WatsonxEmbeddings, WatsonxLLM\n",
    "from langchain_text_splitters.markdown import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langgraph.graph import START, StateGraph\n",
    "from litellm import completion\n",
    "from pydantic import BaseModel, Field\n",
    "from tqdm import tqdm\n",
    "from typing import Literal, Any\n",
    "from typing_extensions import TypedDict\n",
    "import instructor\n",
    "import litellm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WX_API_KEY = config(\"WX_API_KEY\")\n",
    "WX_PROJECT_ID = config(\"WX_PROJECT_ID\")\n",
    "WX_API_URL = \"https://us-south.ml.cloud.ibm.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Authenticate and initialize LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = WatsonxLLM(\n",
    "\n",
    "        model_id= \"ibm/granite-3-8b-instruct\",\n",
    "        url=WX_API_URL,\n",
    "        apikey=WX_API_KEY,\n",
    "        project_id=WX_PROJECT_ID,\n",
    "\n",
    "        params={\n",
    "            GenParams.DECODING_METHOD: \"greedy\",\n",
    "            GenParams.TEMPERATURE: 0,\n",
    "            GenParams.MIN_NEW_TOKENS: 5,\n",
    "            GenParams.MAX_NEW_TOKENS: 1_000,\n",
    "            GenParams.REPETITION_PENALTY:1.2\n",
    "        }\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4762 chunks.\n"
     ]
    }
   ],
   "source": [
    "# Load structured CBS knowledge base\n",
    "df = pd.read_json(\"data/chunked_courses.json\")\n",
    "all_rows = df.to_dict(\"records\")\n",
    "\n",
    "# Prepare LangChain Document objects with metadata\n",
    "chunks = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    programme = row.get(\"programme\", \"\")\n",
    "    course_title = row.get(\"course_title\", \"\")\n",
    "    url = row.get(\"url\", \"\")\n",
    "    language = row.get(\"language\", \"\")\n",
    "    ects = row.get(\"ects\", \"\")\n",
    "    type = row.get(\"type\", \"\")\n",
    "    level = row.get(\"level\", \"\")\n",
    "    study_board = row.get(\"study_board\", \"\")\n",
    "    chunk_text = row.get(\"chunk_text\", \"\").strip()\n",
    "\n",
    "    metadata = {\n",
    "        \"programme\": programme,\n",
    "        \"course_title\": course_title,\n",
    "        \"url\": url,\n",
    "        \"language\": language,\n",
    "        \"ects\": ects,\n",
    "        \"type\": type,\n",
    "        \"level\": level,\n",
    "        \"study_board\": study_board\n",
    "    }\n",
    "    \n",
    "    # Create a Document object for each row\n",
    "    chunks.append(Document(page_content=chunk_text, metadata=metadata))\n",
    "\n",
    "print(f\"Loaded {len(chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_params = {}\n",
    "\n",
    "watsonx_embedding = WatsonxEmbeddings(\n",
    "    model_id=\"ibm/granite-embedding-278m-multilingual\",\n",
    "    url=WX_API_URL,\n",
    "    project_id=WX_PROJECT_ID,\n",
    "    apikey=WX_API_KEY,\n",
    "    params=embed_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create vector index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_vector_db = Chroma.from_documents(\n",
    "    collection_name=\"my_collection\",\n",
    "    embedding=watsonx_embedding,\n",
    "    persist_directory=\"my_vector_db\",\n",
    "    documents=chunks,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a RAG prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a helpful assistant answering questions about CBS graduate programmes. Use the provided context to generate a clear and accurate response. If the answer is not in the context, say you don't know. Keep the answer concise—no more than three sentences.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Context: \n",
    "{context} \n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    \"\"\" A langgraph state for the application \"\"\"\n",
    "    question: str\n",
    "    context: list[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    \"\"\" Our retrieval step. We use our local vector database to retrieve similar documents to the question \"\"\"\n",
    "    retrieved_docs = local_vector_db.similarity_search(state[\"question\"], k=13)\n",
    "    return {\"context\": retrieved_docs} \n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    \"\"\" Our generation step. We use the retrieved documents to generate an answer to the question \"\"\"\n",
    "\n",
    "    # Format the prompt\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    formated_prompt = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "\n",
    "    # Generate the answer\n",
    "    response = llm.invoke(formated_prompt)\n",
    "    return {\"answer\": response}\n",
    "\n",
    "\n",
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\") # Start at the retrieve step\n",
    "graph = graph_builder.compile() # Compile the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Question-Answer pairs (Gold standard examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_queries = [\n",
    "    \"What is the course Organizational Behaviour about?\",\n",
    "    \"Which programme includes the course Organizational Behaviour?\",\n",
    "    \"What are the learning objectives of the Organizational Behaviour course?\",\n",
    "    \"What teaching methods are used in Organizational Behaviour?\",\n",
    "    \"How is the Organizational Behaviour course assessed?\",\n",
    "    \"What is the course Strategic Management about?\",\n",
    "    \"Which programme includes the course Strategic Management?\",\n",
    "    \"What are the learning objectives of the Strategic Management course?\",\n",
    "    \"What teaching methods are used in Strategic Management?\",\n",
    "    \"How is the Strategic Management course assessed?\",\n",
    "    \"What is the course Entrepreneurship and Innovation in Context about?\",\n",
    "    \"Which programme includes the course Entrepreneurship and Innovation in Context?\",\n",
    "    \"What are the learning objectives of the Entrepreneurship and Innovation in Context course?\",\n",
    "    \"What teaching methods are used in Entrepreneurship and Innovation in Context?\",\n",
    "    \"How is the Entrepreneurship and Innovation in Context course assessed?\",\n",
    "    \"What is the course Bioentrepreneurship about?\",\n",
    "    \"What is the course Strategic Management of Innovation and Technology about?\",\n",
    "    \"What is the course Organizational Behavior: Arts and Culture about?\",\n",
    "    \"What is the course Financing Innovation and Entrepreneurship about?\",\n",
    "    \"What is the course Entrepreneurship and Innovation - a Business Game about?\"\n",
    "\n",
    "]\n",
    "\n",
    "expected_responses = [\n",
    "    \"It teaches students how organizations work based on organization and management scholarship.\",\n",
    "    \"This course is part of the BSc in International Business and Politics programme.\",\n",
    "    \"Students learn to understand organizational behavior concepts and apply them to various organizational settings.\",\n",
    "    \"The course uses a range of teaching methods and course materials to introduce important approaches, concepts, and frameworks.\",\n",
    "    \"Assessment is based on a home assignment - written product, with a maximum of 10 pages, to be completed individually within 72 hours.\",\n",
    "    \"It introduces students to the key principles of strategic management, focusing on strategic analysis and contemporary topics in strategy.\",\n",
    "    \"This course is part of the MSc in Business Administration and Commercial Law programme.\",\n",
    "    \"Students learn to master main strategic models at both theoretical and applied levels and apply strategy to real-life cases.\",\n",
    "    \"The course includes lectures, case studies, and discussions to engage students in the classroom.\",\n",
    "    \"Assessment details are provided during the course, typically involving oral exams based on mini-projects and syllabus questions.\",\n",
    "    \"It explores entrepreneurship and innovation and the conditions which economic, societal, and cultural context sets for entrepreneurial decision-making.\",\n",
    "    \"This course is part of the MSc in Social Sciences in Organisational Innovation and Entrepreneurship programme.\",\n",
    "    \"Students learn to understand the impact of context on entrepreneurial decisions and analyze different industries and transitions.\",\n",
    "    \"The course uses case studies, discussions, and analysis of entrepreneurs acting in different industries.\",\n",
    "    \"Assessment details are provided during the course, typically involving written assignments and participation.\",\n",
    "    \"It teaches central skills in science-based entrepreneurship in the biotech sector, bridging science and business.\",\n",
    "    \"It focuses on applying theoretical knowledge to specific problem situations, business challenges, and cases, emphasizing AI and digital platforms.\",\n",
    "    \"It highlights particularities of organizational behavior in the specific case of arts and culture through global examples.\",\n",
    "    \"It helps students analyze entrepreneurial and innovative projects associated with high ambiguity and discusses inequalities in financing.\",\n",
    "    \"It uses a business game platform to apply theories of entrepreneurship and innovation in a practical setting, teaching students to generate and implement new solutions.\"\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a vanilla RAG evaluation system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create a litellm client\n",
    "litellm.drop_params = True  # watsonx.ai doesn't support `json_mode`\n",
    "client = instructor.from_litellm(completion, mode=Mode.JSON)\n",
    "\n",
    "# create a response model - LLM is forced to return an object of this type\n",
    "class JudgeResponse(BaseModel):\n",
    "    reasoning: str = Field(description=\"Short one-sentence reason for score\")\n",
    "    score: Literal[0, .5, 1] = Field(description=\"Final score\")\n",
    "\n",
    "# define a function to call the judge\n",
    "def call_judge(prompt : str) -> JudgeResponse:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"watsonx/meta-llama/llama-3-3-70b-instruct\",\n",
    "        max_tokens=1024,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        project_id=WX_PROJECT_ID,\n",
    "        apikey=WX_API_KEY,\n",
    "        api_base=WX_API_URL,\n",
    "        response_model=JudgeResponse,\n",
    "        # decoding_method=\"greedy\",\n",
    "        # temperature=0,\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGEvaluator:\n",
    "    \"\"\"\n",
    "    A streamlined evaluator for RAG systems focusing on three key dimensions:\n",
    "    1. Retrieval Quality\n",
    "    2. Answer Correctness\n",
    "    3. Hallucination Detection\n",
    "    \"\"\"\n",
    "    def __init__(self, llm_func):\n",
    "        \"\"\"Initialize with an LLM.\"\"\"\n",
    "        self.llm_func = llm_func\n",
    "            \n",
    "    def evaluate_retrieval_quality(self, response: dict[str, Any], expected_answer: str, verbose : bool = False) -> JudgeResponse:\n",
    "        \"\"\"\n",
    "        Ask LLM if retrieved documents contain information needed for the expected answer.\n",
    "        \"\"\"\n",
    "        # Combine all retrieved document contents with clear formatting\n",
    "        retrieved_text = \"\\n\\n\".join([f\"Document {i+1}:\\n{doc.page_content}\" for i, doc in enumerate(response.get('context', []))])\n",
    "        \n",
    "        prompt = f\"\"\"You are given a set of documents about master programs at CBS and a fact. Can the fact be found in the documents? Judge by the information, not the exact wording of the fact.\n",
    "        \n",
    "        - Respond with 1 if the fact is present (also if the fact can be pieced together from multiple documents).\n",
    "        - Respond with 0 if the fact is not present in any of the documents.\n",
    "        - Responds with 0.5 ff only part of the fact is present.\n",
    "        \n",
    "        Retrieved Documents: \n",
    "        {retrieved_text}\n",
    "\n",
    "        Fact:\n",
    "        {expected_answer}\n",
    "        \n",
    "        Can the fact be found in the documents? Respond as a JudgeResponse object with: \n",
    "        - a short reason (max 20 words)\n",
    "        - a score of 1, 0.5, or 0.\n",
    "        \"\"\"\n",
    "        \n",
    "        result = self.llm_func(prompt)\n",
    "        if verbose:\n",
    "            print(f\"[evaluation_retrieval_quality] LLM response: {result}\")\n",
    "        return result\n",
    "    \n",
    "    def evaluate_answer_correctness(self, response: dict[str, Any], expected_answer: str, verbose : bool = False) -> JudgeResponse:\n",
    "        \"\"\"\n",
    "        Ask LLM to rate how correct/similar the generated answer is to the expected answer.\n",
    "        \"\"\"\n",
    "        generated_answer = response.get('answer', '')\n",
    "        \n",
    "        prompt = f\"\"\"You are evaluating a RAG system with academic knowledge base about CBS Master programs. You are given a question, an expected answer, and a generated answer. Is the generated answer as correct - or close to as correct - as the expected answer? \n",
    "        \n",
    "        - Respond with 1 if the answer is yes (also if the answer is more detailed than expected)\n",
    "        - Respond with 0 if the answer is no. \n",
    "        - respond with 0.5 if the generated answer is partially correct\n",
    "\n",
    "        Question:\n",
    "        {response.get('question', '')}\n",
    "        \n",
    "        Expected answer:\n",
    "        {expected_answer}\n",
    "        \n",
    "        Generated answer:\n",
    "        {generated_answer}\n",
    "        \n",
    "        Is the generated answer correct enough? Consider content correctness rather than exact wording. \n",
    "        Respond as a JudgeResponse object with: \n",
    "        - a short reason (max 20 words)\n",
    "        - a score of 1, 0.5, or 0.\"\"\"\n",
    "        \n",
    "        result = self.llm_func(prompt)\n",
    "        if verbose:\n",
    "            print(f\"[evaluation_answer_correctness] LLM response: {result}\")\n",
    "        return result\n",
    "    \n",
    "    def evaluate_hallucination(self, response: dict[str, Any], verbose : bool = False) -> JudgeResponse:\n",
    "        \"\"\"\n",
    "        Ask LLM to evaluate if the answer contains hallucinations.\n",
    "        \"\"\"\n",
    "        generated_answer = response.get('answer', '')\n",
    "        retrieved_text = \"\\n\\n\".join([f\"Document {i+1}:\\n{doc.page_content}\" for i, doc in enumerate(response.get('context', []))])\n",
    "        \n",
    "        prompt = f\"\"\"You are evaluating a RAG system with academic knowledge base about CBS Master programs. Your task is to determine if the generated answer contains hallucinations. Hallucinations are any information that is not directly supported by the retrieved documents. Does the generated answer contain hallucinations? \n",
    "        \n",
    "        - If the answer is no, respond with 0. \n",
    "        - If the answer is yes, respond with 1.\n",
    "        - If the answer is partially hallucinated, respond with 0.5. \n",
    "        - If the generated answer states that it does not know, respond with 0.\n",
    "\n",
    "        Question: \n",
    "        {response.get('question', '')}\n",
    "        \n",
    "        Retrieved context (this is all the information the AI had access to):\n",
    "        {retrieved_text}\n",
    "        \n",
    "        Generated answer:\n",
    "        {generated_answer}\n",
    "        \n",
    "        Does the generated answer contain hallucinations? Respond as a JudgeResponse object with: \n",
    "        - a short reason (max 20 words)\n",
    "        - a score of 1, 0.5, or 0.\n",
    "        \"\"\"\n",
    "        \n",
    "        result = self.llm_func(prompt)\n",
    "        if verbose:\n",
    "            print(f\"[evaluation_hallucination] LLM response: {result}\")\n",
    "        return result\n",
    "    \n",
    "    def evaluate(self, response: dict[str, Any], expected_answer: str, verbose : bool = False) -> dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Evaluate a RAG response across all three dimensions.\n",
    "        \"\"\"\n",
    "        # Get scores for each dimension\n",
    "        retrieval_score = self.evaluate_retrieval_quality(response, expected_answer, verbose=verbose)\n",
    "        correctness_score = self.evaluate_answer_correctness(response, expected_answer, verbose=verbose)\n",
    "        hallucination_score = self.evaluate_hallucination(response, verbose=verbose)\n",
    "        \n",
    "        return {\n",
    "            \"query\": response.get(\"question\", \"\"),\n",
    "            \"retrieved_context\": response.get(\"context\", []),\n",
    "            \"generated_answer\": response.get(\"answer\", \"\"),\n",
    "            \"expected_answer\": expected_answer,\n",
    "            \"retrieval_quality\": retrieval_score.score,\n",
    "            \"answer_correctness\": correctness_score.score, \n",
    "            \"hallucination_score\": hallucination_score.score,  # Lower is better\n",
    "\n",
    "            # keep the reasoning for manual inspection\n",
    "            \"retrieval_quality_reasoning\": retrieval_score.reasoning,\n",
    "            \"answer_correctness_reasoning\": correctness_score.reasoning,\n",
    "            \"hallucination_reasoning\": hallucination_score.reasoning\n",
    "        }\n",
    "\n",
    "\n",
    "def evaluate_rag_system(graph, test_queries, expected_responses, evaluator, verbose=False):\n",
    "    \"\"\"\n",
    "    Evaluate a RAG system containing information about CBS master programs on a test set.\n",
    "    \n",
    "    Args:\n",
    "        graph: The LangGraph RAG system with invoke method\n",
    "        test_queries: List of questions to test\n",
    "        expected_responses: List of expected answers\n",
    "        evaluator: The RAG evaluator object\n",
    "        \n",
    "    Returns:\n",
    "        Evaluation results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for query, expected in tqdm(zip(test_queries, expected_responses), total=len(test_queries)):\n",
    "\n",
    "        # Get RAG response\n",
    "        response = graph.invoke({\"question\": query})\n",
    "        \n",
    "        # Evaluate\n",
    "        eval_result = evaluator.evaluate(response, expected, verbose=verbose)\n",
    "        results.append(eval_result)\n",
    "        time.sleep(8)  # Avoid hitting the API too fast\n",
    "    \n",
    "    # Calculate average scores\n",
    "    avg_metrics = {\n",
    "        \"retrieval_quality\": np.mean([r[\"retrieval_quality\"] for r in results]),\n",
    "        \"answer_correctness\": np.mean([r[\"answer_correctness\"] for r in results]),\n",
    "        \"hallucination\": np.mean([r[\"hallucination_score\"] for r in results])\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"individual_results\": results,\n",
    "        \"scores\": avg_metrics,\n",
    "        \"num_queries\": len(test_queries)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = graph.invoke({\"question\": \"What is the course Organizational Behaviour about?\"})\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate_rag_system(\n",
    "    graph, \n",
    "    sample_queries,\n",
    "    expected_responses,\n",
    "    evaluator=RAGEvaluator(llm_func=call_judge),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "results[\"scores\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores(scores):\n",
    "    \"\"\"\n",
    "    Plot the evaluation scores.\n",
    "    \"\"\"\n",
    "    labels = [\"Retrieval Quality\", \"Answer Correctness\", \"Hallucination\"]\n",
    "    scores = [scores[\"retrieval_quality\"], scores[\"answer_correctness\"], scores[\"hallucination\"]]\n",
    "    \n",
    "    _, ax = plt.subplots()\n",
    "    ax.bar(labels, scores)\n",
    "    ax.set_xlabel('Metric')\n",
    "    # set y range to 0-1\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_title('RAG Evaluation Scores')\n",
    "    plt.show()\n",
    "\n",
    "plot_scores(results[\"scores\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
